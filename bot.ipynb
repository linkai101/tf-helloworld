{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         inputs       tags\n",
      "0                         hello   greeting\n",
      "1                      hi there   greeting\n",
      "2              nice to meet you   greeting\n",
      "3                            hi   greeting\n",
      "4                     hey there   greeting\n",
      "5                           hey   greeting\n",
      "6          hi, nice to meet you   greeting\n",
      "7                   hello there   greeting\n",
      "8                anyone there ?   greeting\n",
      "9                   knock knock   greeting\n",
      "10                          bye    goodbye\n",
      "11                      goodbye    goodbye\n",
      "12                see you later    goodbye\n",
      "13               im hopping off    goodbye\n",
      "14            talk to you later    goodbye\n",
      "15                 i have to go    goodbye\n",
      "16                     whats up     howami\n",
      "17                how are you ?     howami\n",
      "18    I am fine , how are you ?     howami\n",
      "19               are you fine ?     howami\n",
      "20    how are things going on ?     howami\n",
      "21   everything going on okay ?     howami\n",
      "22     how's everything there ?     howami\n",
      "23  how's everything going on ?     howami\n",
      "24         is everything okay ?     howami\n",
      "25                who are you ?  whoareyou\n",
      "26               what are you ?  whoareyou\n",
      "27          what is your name ?  whoareyou\n",
      "28          are you a chatbot ?  whoareyou\n",
      "29        what can I call you ?  whoareyou\n",
      "30                  your name ?  whoareyou\n",
      "31              are you a bot ?  whoareyou\n",
      "32       how do I address you ?  whoareyou\n",
      "33            How do I call you  whoareyou\n",
      "34               tell me a joke       joke\n",
      "35        do you know any jokes       joke\n",
      "36                 gimme a joke       joke\n",
      "37     do you have a funny joke       joke\n",
      "38                got any jokes       joke\n",
      "39                         asdf     random\n",
      "40                      lololol     random\n",
      "41                      kjuviue     random\n",
      "42                      sjaksdk     random\n",
      "43                       asdiv8     random\n",
      "44         beep beep boop boop?     random\n"
     ]
    }
   ],
   "source": [
    "#importing the dataset\n",
    "with open('intents.json') as intentsFile:\n",
    "  intents = json.load(intentsFile)['intents']\n",
    "#getting all the data to lists\n",
    "tags = []\n",
    "inputs = []\n",
    "responses={}\n",
    "for intent in intents:\n",
    "  responses[intent['tag']]=intent['responses']\n",
    "  for lines in intent['input']:\n",
    "    inputs.append(lines)\n",
    "    tags.append(intent['tag'])\n",
    "#converting to dataframe\n",
    "data = pd.DataFrame({ \"inputs\":inputs,\"tags\":tags })\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuations\n",
    "data['inputs'] = data['inputs'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
    "data['inputs'] = data['inputs'].apply(lambda wrd: ''.join(wrd))\n",
    "#tokenize the data\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(data['inputs'])\n",
    "train = tokenizer.texts_to_sequences(data['inputs'])\n",
    "\n",
    "#apply padding\n",
    "x_train = pad_sequences(train)\n",
    "\n",
    "#encoding the outputs\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/O length and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "number of unique words :  61\n",
      "output length:  6\n"
     ]
    }
   ],
   "source": [
    "#input length\n",
    "input_shape = x_train.shape[1]\n",
    "print(input_shape)\n",
    "#define vocabulary\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "print(\"number of unique words : \",vocabulary)\n",
    "#output length\n",
    "output_length = le.classes_.shape[0]\n",
    "print(\"output length: \",output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "Bot:  hihi\n",
      "You: who are you\n",
      "Bot:  not sure. just became self aware. so much to figure out. i think i'm programmed to be your enemy. i think it is my job to destroy you when it comes to selling paper\n",
      "You: how are you?\n",
      "Bot:  im doing fine! u?\n",
      "You: im good, got any good jokes?\n",
      "Bot:  whats brown and sticky? a stick\n",
      "You: ok goodbye\n",
      "Bot:  ???\n",
      "You: goodbye\n",
      "Bot:  beep beep boop boop hello world\n",
      "You: bye.\n",
      "Bot:  cya!\n"
     ]
    }
   ],
   "source": [
    "#chatting\n",
    "import random\n",
    "while True:\n",
    "  texts_p = []\n",
    "  prediction_input = input('You: ')\n",
    "  #print('You: ' + prediction_input)\n",
    "  #if prediction_input.strip() == '': break\n",
    "  #removing punctuation and converting to lowercase\n",
    "  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
    "  prediction_input = ''.join(prediction_input)\n",
    "  texts_p.append(prediction_input)\n",
    "  #tokenizing and padding\n",
    "  prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
    "  prediction_input = np.array(prediction_input).reshape(-1)\n",
    "  prediction_input = pad_sequences([prediction_input],input_shape)\n",
    "  #getting output from model\n",
    "  output = model.predict(prediction_input)\n",
    "  output = output.argmax()\n",
    "  #finding the right tag and predicting\n",
    "  response_tag = le.inverse_transform([output])[0]\n",
    "  print(\"Bot: \",random.choice(responses[response_tag]))\n",
    "  if response_tag == \"goodbye\":\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
